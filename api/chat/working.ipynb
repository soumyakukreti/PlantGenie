{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'plantapp' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n plantapp ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\n",
    "llm = Llama(\n",
    "  model_path=\"stablelm-zephyr-3b.Q4_K_M.gguf\",  # Download the model file first\n",
    "  n_ctx=768,  # The max sequence length to use - note that longer sequence lengths require much more resources\n",
    "  n_threads=12,            # The number of CPU threads to use, tailor to your system and the resulting performance\n",
    "  n_gpu_layers=1         # The number of layers to offload to GPU, if you have GPU acceleration available\n",
    ")\n",
    "\n",
    "# Simple inference example\n",
    "output = llm(\n",
    "  \"<|user|>\\nWhat is the capital of Istanbul<|endoftext|>\\n<|assistant|>\", # Prompt\n",
    "  max_tokens=128,  # Generate up to 512 tokens\n",
    "  stop=[\"</s>\"],   # Example stop token - not necessarily correct for this specific model! Please check before using.\n",
    "  echo=True        # Whether to echo the prompt\n",
    ")\n",
    "\n",
    "print(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm = Llama(model_path=\"./stablelm-zephyr-3b.Q4_K_M.gguf\", chat_format=\"llama-2\")  # Set chat_format according to the model you are using\n",
    "llm.create_chat_completion(\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a story writing assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write a story about llamas.\"\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms import LlamaCPP\n",
    "from llama_index.llms.llama_utils import messages_to_prompt, completion_to_prompt\n",
    "from llama_index.embeddings import HuggingFaceEmbedding\n",
    "from llama_index import VectorStoreIndex\n",
    "from llama_index import ServiceContext\n",
    "from llama_index.vector_stores import ChromaVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 21 key-value pairs and 356 tensors from stablelm-zephyr-3b.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = stablelm\n",
      "llama_model_loader: - kv   1:                               general.name str              = source\n",
      "llama_model_loader: - kv   2:                    stablelm.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                  stablelm.embedding_length u32              = 2560\n",
      "llama_model_loader: - kv   4:                       stablelm.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:               stablelm.feed_forward_length u32              = 6912\n",
      "llama_model_loader: - kv   6:              stablelm.rope.dimension_count u32              = 20\n",
      "llama_model_loader: - kv   7:              stablelm.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:             stablelm.use_parallel_residual bool             = true\n",
      "llama_model_loader: - kv   9:      stablelm.attention.layer_norm_epsilon f32              = 0.000010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: - kv  10:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  11:                      tokenizer.ggml.tokens arr[str,50304]   = [\"<|endoftext|>\", \"<|padding|>\", \"!\",...\n",
      "llama_model_loader: - kv  12:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.merges arr[str,50009]   = [\"Ġ Ġ\", \"Ġ t\", \"Ġ a\", \"h e\", \"i n...\n",
      "llama_model_loader: - kv  14:                tokenizer.ggml.bos_token_id u32              = 0\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.eos_token_id u32              = 0\n",
      "llama_model_loader: - kv  16:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:                    tokenizer.chat_template str              = {% for message in messages %}\\n{% if m...\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  20:                          general.file_type u32              = 15\n",
      "llama_model_loader: - type  f32:  130 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: mismatch in special tokens definition ( 31/50304 vs 52/50304 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = stablelm\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 50304\n",
      "llm_load_print_meta: n_merges         = 50009\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 2560\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 20\n",
      "llm_load_print_meta: n_embd_head_k    = 80\n",
      "llm_load_print_meta: n_embd_head_v    = 80\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2560\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2560\n",
      "llm_load_print_meta: f_norm_eps       = 1.0e-05\n",
      "llm_load_print_meta: f_norm_rms_eps   = 0.0e+00\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 6912\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 3B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 2.80 B\n",
      "llm_load_print_meta: model size       = 1.59 GiB (4.88 BPW) \n",
      "llm_load_print_meta: general.name     = source\n",
      "llm_load_print_meta: BOS token        = 0 '<|endoftext|>'\n",
      "llm_load_print_meta: EOS token        = 0 '<|endoftext|>'\n",
      "llm_load_print_meta: UNK token        = 0 '<|endoftext|>'\n",
      "llm_load_print_meta: PAD token        = 0 '<|endoftext|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_tensors: ggml ctx size =    0.14 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  1627.74 MiB\n",
      "............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   160.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  160.00 MiB, K (f16):   80.00 MiB, V (f16):   80.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =     7.01 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   108.25 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.chat_template': \"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'system' %}\\n{{ '<|system|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\", 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '0', 'tokenizer.ggml.model': 'gpt2', 'general.file_type': '15', 'stablelm.attention.layer_norm_epsilon': '0.000010', 'stablelm.attention.head_count': '32', 'stablelm.embedding_length': '2560', 'tokenizer.ggml.padding_token_id': '0', 'stablelm.rope.dimension_count': '20', 'general.architecture': 'stablelm', 'stablelm.block_count': '32', 'stablelm.context_length': '4096', 'tokenizer.ggml.bos_token_id': '0', 'stablelm.feed_forward_length': '6912', 'general.name': 'source', 'stablelm.use_parallel_residual': 'true'}\n",
      "Using chat template: {% for message in messages %}\n",
      "{% if message['role'] == 'user' %}\n",
      "{{ '<|user|>\n",
      "' + message['content'] + eos_token }}\n",
      "{% elif message['role'] == 'system' %}\n",
      "{{ '<|system|>\n",
      "' + message['content'] + eos_token }}\n",
      "{% elif message['role'] == 'assistant' %}\n",
      "{{ '<|assistant|>\n",
      "'  + message['content'] + eos_token }}\n",
      "{% endif %}\n",
      "{% if loop.last and add_generation_prompt %}\n",
      "{{ '<|assistant|>' }}\n",
      "{% endif %}\n",
      "{% endfor %}\n",
      "Using chat eos_token: \n",
      "Using chat bos_token: \n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama(\n",
    "    model_path=\"stablelm-zephyr-3b.Q4_K_M.gguf\",\n",
    "    temperature=0.4,\n",
    "    max_new_tokens=128,\n",
    "    context_window=768,\n",
    "    generate_kwargs={},\n",
    "    messages_to_prompt=messages_to_prompt,\n",
    "    completion_to_prompt=completion_to_prompt,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     729.41 ms\n",
      "llama_print_timings:      sample time =      30.44 ms /   256 runs   (    0.12 ms per token,  8409.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =     729.32 ms /    29 tokens (   25.15 ms per token,    39.76 tokens per second)\n",
      "llama_print_timings:        eval time =    9370.84 ms /   255 runs   (   36.75 ms per token,    27.21 tokens per second)\n",
      "llama_print_timings:       total time =   10702.67 ms /   284 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "What do you know about snake plant and can I use them as potted plants<|endoftext|>\n",
      "<|assistant|> Snake plants, also known as Sansevieria or Sansevieriaceae, are a popular indoor plant family that belongs to the Asparagaceae (formerly classified under Ginkgoaceae) group. Trees typically have variegated leaves with vertical stripes or bands of different colors, although some species may have solid green leaves. They originate from tropical regions in Africa, South America, and Southeast Asia.\n",
      "\n",
      "Snake plants are well-known for their air-purifying qualities. They remove toxins such as formaldehyde, benzene, and trichloroethylene from the air, making them an ideal choice for spaces with poor air quality. They also help regulate humidity levels in your home.\n",
      "\n",
      "Yes, snake plants can be used as potted plants. They come in various shapes and sizes, including tall, trailing, and compact varieties. They're easy to care for and suitable for various growing conditions, making them a great choice for people who want low-maintenance indoor plants. To care for a snake plant in a pot:\n",
      "\n",
      "1. Lighting: Snake plants prefer bright, indirect sunlight. Place your pot near an east-facing window, where it receives plenty of indirect sunlight. Avoid direct sunlight during peak hours (typically midday\n"
     ]
    }
   ],
   "source": [
    "output = llm(\n",
    "  \"<|user|>\\nWhat do you know about snake plant and can I use them as potted plants<|endoftext|>\\n<|assistant|>\", # Prompt\n",
    "  max_tokens=256,  # Generate up to 512 tokens\n",
    "  stop=[\"</s>\"],   # Example stop token - not necessarily correct for this specific model! Please check before using.\n",
    "  echo=True        # Whether to echo the prompt\n",
    ")\n",
    "\n",
    "final = output['choices'][0][\"text\"]\n",
    "\n",
    "print(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
